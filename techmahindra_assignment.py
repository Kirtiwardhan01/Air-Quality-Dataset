# -*- coding: utf-8 -*-
"""TechMahindra assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GFE-UVlaPC5tz0KzDmZ9GEmM9uTZNnQw
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

"""Reading the data"""

AQ = pd.read_csv(r"C:\Users\hp\Desktop\kirtiwardhan\Data Science task\Recruitment task\AirQualityUCI.csv")
print(AQ.head())

#Split hour from time into new column
AQ['Hour']=AQ['Time'].apply(lambda x: int(x.split('.')[0]))
AQ.Hour.head()

AQ.info()

AQ.head()

"""About Missing Data From data description, Missing values are tagged with -200 value. Let's check missing data"""

AQ = AQ.replace(-200,np.nan)

AQ_missing = AQ.isnull().sum().sort_values(ascending=False)
AQ_missing = pd.DataFrame(AQ_missing).reset_index()
AQ_missing.columns = ['Feature', 'Number of Data Records Missing']
AQ_missing.head()

"""We first try to clean the data by visualising any NA/null values in the data set"""

sns.heatmap(AQ.isnull(),yticklabels=False,cbar=False,cmap='viridis')

AQ.info()

"""Visualize the percentage of 5 features with most data missing"""

AQ_missingpercent = 100 * (AQ.isnull().sum()/AQ.isnull().count()).sort_values(ascending=False)

plt.Figure(figsize=(7,7),dpi = 400)
plt.bar(np.arange(5), AQ_missingpercent[:5], align='center', alpha=0.5)
plt.xticks(np.arange(5), AQ_missingpercent.index[:5], fontsize=12)
plt.ylim(0, 100)
plt.ylabel('Percentage of Data Missing', fontsize = 14)
plt.title('5 Features with Most Data Missing in Dataset', fontsize = 14) 
# plt.savefig('5 Features with Most Data Missing in Dataset.jpg')
plt.show()

"""Drop Redundant Features & Replace Missing Data

Feature 'NMHC(GT)' has more than 80% data missing, drop it

Thus we can remove this column from the dataset as these values are very less likely to be of any significant importance in this dataset

Analyze the correlations of all remaining features, drop redundant features

Replace missing data in remaining features
"""

AQ = AQ.drop(['NMHC(GT)'], axis = 1)

AQ['T'].value_counts(dropna= False)

AQ['CO(GT)'].value_counts(dropna= False)

AQ['NOx(GT)'].value_counts(dropna= False)

AQ['NO2(GT)'].value_counts(dropna= False)

"""We can replace these NaN values by taking the mean whole column, but this won't be accurate and proper way to fill those values. 
Thus, filling the mean of that perticular day in which day the value is NaN makes more sense and would be proper way to impute the values
"""

AQ["T"] = AQ.groupby("Date")["T"].transform(lambda x: x.fillna(x.mean()))

AQ["CO(GT)"] = AQ.groupby("Date")["CO(GT)"].transform(lambda x: x.fillna(x.mean()))

AQ["NOx(GT)"] = AQ.groupby("Date")["NOx(GT)"].transform(lambda x: x.fillna(x.mean()))
AQ["NO2(GT)"] = AQ.groupby("Date")["NO2(GT)"].transform(lambda x: x.fillna(x.mean()))

sns.heatmap(AQ.isnull(),yticklabels=False,cbar=False,cmap='viridis')

"""We can see there are still some NaN values, this is because for these values. We fill these values using Forward Fill way inside the fillna function, using method 'ffill'. 

We use forward fill here as even now taking the mean of the whole column(which has values for an entire year) does not make sense. 

Hence as these are hourly values and thus forward fill will be a very good method to fill thses values instead of dropping these records.
"""

AQ.fillna(method='ffill', inplace= True)

AQ.isnull().any()

AQ.info()

"""The describe method is used to find the summary statistics for every column

Now that the data is all cleaned and free of NaN values, we can start the with plots and Exploratory Analysis
"""

AQ.plot(x='Date',subplots=True, figsize=(15, 10),legend=False)

"""distplots for all the columns. Distplot provides a quick way to look at the univariate distribution."""

sns.distplot(AQ["NOx(GT)"])

sns.distplot(AQ["NOx(GT)"])

sns.distplot(AQ["CO(GT)"])

sns.distplot(AQ["NO2(GT)"])

sns.distplot(AQ["C6H6(GT)"])

sns.distplot(AQ["T"])

sns.distplot(AQ["RH"])

sns.distplot(AQ["AH"])

AQ.describe()

AQ['Date']=pd.to_datetime(AQ.Date, format='%d/%m/%Y')   #Format date column

# set the index as date              #Create month column (Run once)
AQ.set_index('Date',inplace=True)

AQ['Month']=AQ.index.month     #Create month column (Run once)
AQ.reset_index(inplace=True)
#AQ.head()

"""# Understand co-relation between variables"""

sns.heatmap(AQ.corr(), annot=True)

#correlation values table
print(AQ.corr())

#plot all X-features against output variable RH
col_=AQ.columns.tolist()[2:]
for i in AQ.columns.tolist()[2:]:
    sns.lmplot(x=i,y='RH',data=AQ,markers='.')

from sklearn.preprocessing import StandardScaler         #import normalisation package
from sklearn.model_selection import train_test_split    
from sklearn.linear_model import LinearRegression         
from sklearn.metrics import mean_squared_error,mean_absolute_error

X=AQ[col_].drop('RH',1)     #X-input features
y=AQ['RH']                    #y-input features

"""# The basic of hypothesis is normalisation and standard normalisation

Normalize Feature variable
"""

scaler=StandardScaler()     #Creating an instance

X_std=scaler.fit_transform(X)     #apply standardisation

X.shape,y.shape

"""#Split the data into train and test with test size and 20% and train size as 80%"""

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.2,random_state=7)

print('Training data size:',X_train.shape)
print('Test data size:',X_test.shape)

reg = LinearRegression()

"""Train the model"""

AQ_model= reg.fit(X_train,y_train)   # fit linear model
AQ_model

print('Intercept:',AQ_model.intercept_)
print('--------------------------------')
print('Slope:')
list(zip(X.columns.tolist(),AQ_model.coef_))

"""Doing Prediction"""

y_pred=AQ_model.predict(X_test)                      #predict using the model
RMSE=np.sqrt(mean_squared_error(y_test,y_pred))      #calculate rmse
print('Baseline RMSE of model:',RMSE)

"""We can predict RH using all the features together with RMSE as 6.631 

Let us call it as baseline model

We will try with multiple feature combinations and see if RMSE is improving
"""

# write function to measure RMSE
def train_test_RMSE(feature):
    X=AQ[feature]
    y=AQ['RH']
    X_std_one=scaler.fit_transform(X)
    X_trainR,X_testR,y_trainR,y_testR=train_test_split(X_std_one,y,test_size=0.2,random_state=7)
    AQ_model_one=reg.fit(X_trainR,y_trainR)
    y_predR=AQ_model_one.predict(X_testR)
    return np.sqrt(mean_squared_error(y_testR,y_predR))

col_.remove('RH')        #remove output

print('List of features:',col_)    #print list of features

print('RMSE with Features as',col_[0:2],train_test_RMSE(col_[0:2]))
print('-------------------------')
print('RMSE with Features as',col_[0:6],train_test_RMSE(col_[0:6]))
print('-------------------------')
print('RMSE with Features as',col_[0:9],train_test_RMSE(col_[0:9]))
print('-------------------------')
print('RMSE with Features as',col_[1:5],train_test_RMSE(col_[2:9]))
print('-------------------------')
print('RMSE with Features as',col_[0:11],train_test_RMSE(col_[0:11]))
print('-------------------------')
print('RMSE with Features as',col_[1:12],train_test_RMSE(col_[1:12]))
print('-------------------------')
print('RMSE with Features as',col_[0:13],train_test_RMSE(col_[0:13]))

"""After this experiment it looks that baseline model is performing best"""

from sklearn.model_selection import GridSearchCV        #import grid search cv

"""Let us apply Random Forest regression and measure RMSE

Fit the RF model and predict
"""

from sklearn.ensemble import RandomForestRegressor           #import random forest regressor
rf_reg=RandomForestRegressor()

rf_model=rf_reg.fit(X_train,y_train)         #fit model   
y_pred_rf=rf_model.predict(X_test)           #predict

#Calculating RMSE
print('RMSE of predicted RH in RF model:',np.sqrt(mean_squared_error(y_test,y_pred_rf)))

"""Lets try to improve on baseline RF model"""

#define rf parameters
rf_params={'n_estimators':[10,20],'max_depth':[8,10],'max_leaf_nodes':[70,90]}
#define rf grid search
rf_grid=GridSearchCV(rf_reg,rf_params,cv=10)

rf_model_two=rf_grid.fit(X_train,y_train)     #fit the model wtih all grid parameters
rf_model_two

y_pred_rf_two=rf_model_two.predict(X_test)        #predict
y_pred_rf_two

#Calculate RMSE
print('RMSE using RF grid search method',np.sqrt(mean_squared_error(y_test,y_pred_rf_two)))

"""Applying Random Forest regression the predicted RMSE has improved to 0.97, 

The default RF algorithm is giving better RMSE value than grid search applied different parameters

Let us try to apply Decision tree regression technique and see if any improvement happens
"""

from sklearn.tree import DecisionTreeRegressor         #Decision tree regression model
from sklearn.model_selection import cross_val_score    #import cross validation score package
from sklearn.model_selection import GridSearchCV        #import grid search cv
dt_one_reg=DecisionTreeRegressor()

"""Fit the DT model and predict:"""

dt_model=dt_one_reg.fit(X_train,y_train)         #fit the model
y_pred_dtone=dt_model.predict(X_test)            #predict

"""RMSE of RH prediction"""

#calculate RMSE
print('RMSE of Decision Tree Regression:',np.sqrt(mean_squared_error(y_pred_dtone,y_test)))

"""For designing the model for predicting RH, I have applied Linear Regression, Decision Tree, Random Forest

When tested on test data below are RMSE obtained from different algorithms:

RMSE:

-Linear Regression: 6.63

-Decision Tree: 1.32

-Random Forest: 0.97

We can compare the RMSE values and conclude that Random Forest algorithm is better algorithm for predicting RH values using the features
"""



